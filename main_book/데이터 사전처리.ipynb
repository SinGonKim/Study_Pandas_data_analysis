{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ebeb44",
   "metadata": {},
   "source": [
    "# 누락 데이터 처리\n",
    "수집한 데이터를 분석에 적합하도록 사전 처리(preprocessing)하는 방법을 살펴보자.  \n",
    "일반적으로 유효한 데이터 값이 존재하지 않는 누락 데이터를 NaN(Not a Number)으로 표시한다.  \n",
    "머신러닝 분석 모형에 데이터를 입력하기 전에 반드시 누락 데이터를 제거하거나 다른 적절한 값으로 대체하는 과정이 필요하다.  \n",
    "누락 데이터가 많아지면 데이터의 품질이 떨어지고, 머신러닝 분석 알고리즘을 왜곡하는 현상이 발생하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e85e17f",
   "metadata": {},
   "source": [
    "### 누락 데이터 확인\n",
    "Seaborn 라이브러리의 `titanic` 데이터셋을 사용한다.  \n",
    "head() 메소드로 첫 5행을 출력하면 `deck`열에 NaN 값이 있다. 이 승객의 경우 몇 번 테크에 승선했는지 데이터가 없다는 뜻이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6692106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceea9205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48822337",
   "metadata": {},
   "source": [
    "info() 메소드로 데이터프레임의 요약 정보를 출력하면 각 열에 속하는 데이터 중에서 유효한(NaN 값이 아닌)값의 개수를 보여준다.\n",
    "`RangeIndex`를 보면 각 열에 891개의 데이터가 있다. 그리고 `deck`열에는 203개의 유효한(non-null) 범주형(category)데이터가 있다.  \n",
    "따라서 `deck` 열에 있는 누락 데이터가 688개라는 사실을 알 수 있다.(계산식: 891-203=698)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4de710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c5691",
   "metadata": {},
   "source": [
    "value_count() 메소드를 이용하여 `deck` 열에 688개의 누락 데이터가 있는 것을 파악할 수도 있다.  \n",
    "단, 이때 누락 데이터의 개수를 확인하려면 반드시 `dropna=False` 옵션을 사용한다.  \n",
    "그렇지 않으면 NaN 값을 제외하고 유효한 데이터의 개수만을 구하기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90564da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN    688\n",
      "C       59\n",
      "B       47\n",
      "D       33\n",
      "E       32\n",
      "A       15\n",
      "F       13\n",
      "G        4\n",
      "Name: deck, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# deck 열의 NaN 개수 계산하기\n",
    "nan_deck = df['deck'].value_counts(dropna=False)\n",
    "print(nan_deck)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce21e26b",
   "metadata": {},
   "source": [
    "누락 데이터를 찾는 직접적인 방법으로 isnull() 메소드와 notnull() 메소드가 있다.\n",
    "- isnull() : 누락 데이터면 True를 반환하고, 유효한 데이터가 존재하면 False를 반환한다.\n",
    "- notnull() : 유효한 데이터가 존재하면 True를 반환하고, 누락 데이터면 False를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edd68c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass    sex    age  sibsp  parch   fare  embarked  class  \\\n",
      "0     False   False  False  False  False  False  False     False  False   \n",
      "1     False   False  False  False  False  False  False     False  False   \n",
      "2     False   False  False  False  False  False  False     False  False   \n",
      "3     False   False  False  False  False  False  False     False  False   \n",
      "4     False   False  False  False  False  False  False     False  False   \n",
      "\n",
      "     who  adult_male   deck  embark_town  alive  alone  \n",
      "0  False       False   True        False  False  False  \n",
      "1  False       False  False        False  False  False  \n",
      "2  False       False   True        False  False  False  \n",
      "3  False       False  False        False  False  False  \n",
      "4  False       False   True        False  False  False  \n"
     ]
    }
   ],
   "source": [
    "# isnull() 메소드로 누락 데이터 찾기\n",
    "print(df.head().isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "715b4fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass   sex   age  sibsp  parch  fare  embarked  class   who  \\\n",
      "0      True    True  True  True   True   True  True      True   True  True   \n",
      "1      True    True  True  True   True   True  True      True   True  True   \n",
      "2      True    True  True  True   True   True  True      True   True  True   \n",
      "3      True    True  True  True   True   True  True      True   True  True   \n",
      "4      True    True  True  True   True   True  True      True   True  True   \n",
      "\n",
      "   adult_male   deck  embark_town  alive  alone  \n",
      "0        True  False         True   True   True  \n",
      "1        True   True         True   True   True  \n",
      "2        True  False         True   True   True  \n",
      "3        True   True         True   True   True  \n",
      "4        True  False         True   True   True  \n"
     ]
    }
   ],
   "source": [
    "# notnull() 메소드로 누락 데이터 찾기\n",
    "print(df.head().notnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8dd46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "deck           3\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# isnull() 메소드로 누락 데이터 개수 구하기\n",
    "print(df.head().isnull().sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23ef40",
   "metadata": {},
   "source": [
    "### 누락 데이터 제거\n",
    "누락 데이터가 들어 있는 열 또는 행을 삭제하는 방법을 알아보자.  \n",
    "열을 삭제하면 분석 대상이 갖는 특성(변수)를 제거하고, 행을 삭제하면 분석 대상의 간측값(레코드)을 제거하게 된다.\n",
    "\n",
    "먼저 `titanic` 데이터셋의 각 열(변수)에 누락 데이터가 몇 개씩 포함되어 있는지 체크한다.  \n",
    "isnull() 메소드와 value_counts() 메소드를 적용한 결과, `age` 열에 177개, `embarked`열에 2개, `deck` 열에 688개, `embark_town`열에 2개가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67ef299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived : 0\n",
      "pclass : 0\n",
      "sex : 0\n",
      "age : 177\n",
      "sibsp : 0\n",
      "parch : 0\n",
      "fare : 0\n",
      "embarked : 2\n",
      "class : 0\n",
      "who : 0\n",
      "adult_male : 0\n",
      "deck : 688\n",
      "embark_town : 2\n",
      "alive : 0\n",
      "alone : 0\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# for 반복문으로 각 열의 NaN 개수 계산하기\n",
    "missing_df = df.isnull()\n",
    "for col in missing_df.columns:\n",
    "    missing_count = missing_df[col].value_counts()\n",
    "    \n",
    "    try:\n",
    "        print(col, ':', missing_count[True]) # NaN 값이 있으면 개수 출력\n",
    "    except:\n",
    "        print(col, ':', 0) #NaN 값이 없으면 0개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "572ad9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'embark_town', 'alive',\n",
      "       'alone'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# NaN 값이 500개 이상인 열을 모두 삭제 - deck 열 (891개 중 688개의 NaN 값)\n",
    "df_thresh = df.dropna(axis=1, thresh=500)\n",
    "print(df_thresh.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c17aa",
   "metadata": {},
   "source": [
    "dropna() 메소드에 subset을 `age`열로 한정하면, `age`열의 행 중에서 NaN 값이 있는 모든 행(axis=0)을 삭제한다.  \n",
    "기본값으로 how='any' 옵션이 적용되는데, NaN 값이 하나라도 존재하면 삭제한다는 뜻이다. how='all' 옵션으로 입력하면 모든 데이터가 NaN값일 경우에만 삭제가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6116d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n"
     ]
    }
   ],
   "source": [
    "# age 열에 나이 데이터가 없는 모든 행 삭제 - age열 (891개 중 177개의 NaN값)\n",
    "df_age = df.dropna(subset=['age'], how='any', axis = 0)\n",
    "print(len(df_age))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1dc9d7",
   "metadata": {},
   "source": [
    "### 누락 데이터 치환\n",
    "데이터셋의 품질을 높을 목적으로 누락 데이터를 무작정 삭제해 버린다면 어렵게 수집한 데이터를 활용하지 못하게 된다.  \n",
    "데이터 분석의 정확도는 데이터의 품질 외에도 제공되는 데이터의 양에 의해 상당히 영향을 받는다.  \n",
    "따라서 데이터 중에서 일부가 누락되어 있더라도 나머지 데이터를 최대한 살려서 데이터 분석에 활용하는 것이 좋은 결과를 얻는 경우가 많다.  \n",
    "\n",
    "누락 데이터를 바꿔서 대체할 값으로는 데이터의 분포와 특성을 잘 나타낼 수 있는 평균값, 최빈값 등을 활용한다.  \n",
    "판다스 fillna() 메소드로 편리하게 처리할 수 있다. fillna() 메소드는 새로운 객체를 반환하기 때문에 원본 객체를 변경하려면 `inplace=True` 옵션을 추가해야한다.  \n",
    "\n",
    "평균(mean)으로 누락 데이터를 바꿔서 하는 방법도 있다. 앞의 예제처럼 승객의 나이 데이터가 누락된 행을 제거하지 않고, 대신 `age` 열의 나머지 승객의 평균 나이로 치환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f756dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22.0\n",
      "1    38.0\n",
      "2    26.0\n",
      "3    35.0\n",
      "4    35.0\n",
      "5     NaN\n",
      "6    54.0\n",
      "7     2.0\n",
      "8    27.0\n",
      "9    14.0\n",
      "Name: age, dtype: float64\n",
      "\n",
      "\n",
      "0    22.000000\n",
      "1    38.000000\n",
      "2    26.000000\n",
      "3    35.000000\n",
      "4    35.000000\n",
      "5    29.699118\n",
      "6    54.000000\n",
      "7     2.000000\n",
      "8    27.000000\n",
      "9    14.000000\n",
      "Name: age, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# age 열의 첫 10개 데이터 출력(5행에 NaN 값)\n",
    "print(df['age'].head(10))\n",
    "print('\\n')\n",
    "\n",
    "# age 열의 NaN 값을 다른 나이 데이터의 평균으로 변경하기\n",
    "mean_age = df['age'].mean(axis=0) # age 열의 평균 계산 (NaN 값 제외)\n",
    "df['age'].fillna(mean_age, inplace=True)\n",
    "\n",
    "# age 열의 첫 10개 데이터 출력(5행에 NaN 값이 평균으로 대체)\n",
    "print(df['age'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dbb046",
   "metadata": {},
   "source": [
    "승선도시를 나타내는 `embark_town` 열에 있는 NaN을 다른 값으로 바꾼다. 승객들이 가장 많이 승선한 도시의 이름을 찾아서 NaN을 치환한다.  \n",
    "먼저 value_count() 메소드를 사용하여 승선도시별 승객 수를 찾고, idxmax() 메소드로 가장 큰 값을 갖는 도시를 찾는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d1323cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "\n",
      "Southampton\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829    Southampton\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# embark_town 열의 829행의 NaN 데이터 출력\n",
    "print(df['embark_town'][825:830])\n",
    "print('\\n')\n",
    "\n",
    "# embark_town 열의 NaN 값을 승선도시 중에서 가장 많이 출현한 값으로 치환하기\n",
    "most_freq = df['embark_town'].value_counts(dropna=True).idxmax()\n",
    "print(most_freq)\n",
    "print('\\n')\n",
    "\n",
    "df['embark_town'].fillna(most_freq, inplace=True)\n",
    "\n",
    "# embark_town 열 829행의 NaN 데이터 출력(NaN 값이 most_freq 값으로 대체)\n",
    "print(df['embark_town'][825:830])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d768fdf",
   "metadata": {},
   "source": [
    "### 누락 데이터가 NaN으로 표시되지 않은 경우\n",
    "데이터셋 중에는 누락 데이터가 NaN으로 입력되지 않은 경우도 많다.  \n",
    "예를 들면, 숫자 0이나 문자 '-', '?' 같은 값으로 입력되기도 한다.  \n",
    "판다스에서 누락 데이터를 다루려면 replace() 메소드를 활용하여 NumPy에서 지원하는 np.nan으로 변경해 주는 것이 좋다.  \n",
    "단 np.nan을 사용하기 위해서는 'import numpy as np'와 같이 NumPy 라이브러리를 먼저 임포트해야한다.  \n",
    "- 사용법 예('?'을 np.nan으로 치환): df.replace('?', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aff6f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829            NaN\n",
      "Name: embark_town, dtype: object\n",
      "\n",
      "\n",
      "825     Queenstown\n",
      "826    Southampton\n",
      "827      Cherbourg\n",
      "828     Queenstown\n",
      "829     Queenstown\n",
      "Name: embark_town, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# titanic 데이터셋 가져오기\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# embark_town 열 829행의 NaN 데이터 출력\n",
    "print(df['embark_town'][825:830])\n",
    "print('\\n')\n",
    "\n",
    "# embark_town 열의 NaN 값을 바로 앞에 있는 828행의 값으로 변경하기\n",
    "df['embark_town'].fillna(method='ffill', inplace=True)\n",
    "print(df['embark_town'][825:830])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e606900",
   "metadata": {},
   "source": [
    "# 중복 데이터 처리\n",
    "\n",
    "### 중복 데이터 확인\n",
    "동일한 관측값이 중복되는지 여부, 즉 행의 레코드가 중복되는지 여부를 확인하려면 duplicated() 메소드를 이용한다. 전에 나온 행들과 비교하여 중복되는 행이면 True를 반환하고, 처음 나오는 행에 대해서는 False를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7477b8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c\n",
      "0  a   1  1\n",
      "1  a   1  1\n",
      "2  b   1  2\n",
      "3  a   2  2\n",
      "4  b   2  2\n",
      "\n",
      "\n",
      "0    False\n",
      "1     True\n",
      "2    False\n",
      "3    False\n",
      "4    False\n",
      "dtype: bool\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 중복 데이터를 갖는 데이터프레임 만들기\n",
    "df = pd.DataFrame({'c1':['a', 'a', 'b', 'a', 'b'],\n",
    "                  'c2': [1,1,1,2,2], \n",
    "                  'c' : [1,1,2,2,2]})\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임 전체 행 데이터 중에서 중봅값 찾기\n",
    "df_dup = df.duplicated()\n",
    "print(df_dup)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3def44a",
   "metadata": {},
   "source": [
    "데이터프레임의 열은 시리즈 객체이므로, duplicated() 메소드를 적용 할 수 있다.  \n",
    "데이터프레임 df의 `c2` 열은 정수 1과 2로 구성된다. 1이 처음 나타난 0행과 2가 처음 나타난 3행을 제외하고 나머지 1, 2, 4행은 이전에 나온 행과 중복되므로 True가 된다.  \n",
    "1, 2행은 데이터 1을 가진 0행과 중복되고, 4행은 데이터 2를 가진 3행과 중복된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d9ebad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "1     True\n",
      "2     True\n",
      "3    False\n",
      "4     True\n",
      "Name: c2, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임의 특정 열 데이터에서 중복값 찾기\n",
    "col_dup = df['c2'].duplicated()\n",
    "print(col_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0d70f",
   "metadata": {},
   "source": [
    "### 중복 데이터 제거\n",
    "중복 데이터를 제거하는 명령에는 drop_duplicates() 메소드가 있다. 중복되는 행을 제거하고 고유한 관측값을 가진 행들만 남기다.  \n",
    "원본 객체를 변경하려면 inplace=True 옵션을 추가한다.  \n",
    "다음의 예제에서 1행의 데이터는 앞에 이웃하고 있는 0행의 데이터와 중복되므로 제거된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56cc2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "1  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n",
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n",
      "4  b   2   2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 중복 데이터를 갖는 데이터프레임 만들기\n",
    "df = pd.DataFrame({'c1': ['a', 'a', 'b', 'a', 'b'],\n",
    "                  'c2': [1, 1, 1, 2, 2],\n",
    "                  'c3' : [1, 1, 2, 2, 2]})\n",
    "print(df)\n",
    "print('\\n')\n",
    "\n",
    "# 데이터프레임에서 중복 행 제거\n",
    "df2 = df.drop_duplicates()\n",
    "print(df2)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe2eaf",
   "metadata": {},
   "source": [
    "drop_duplicates() 메소드의 subset 옵션에 '열 이름의 리스트'를 전달할 수 있다.  \n",
    "데이터의 중복 여부를 판별할 때, subset 옵션에 해당하는 열을 기준으로 판단한다.  \n",
    "데이터프레임 df의 'c2', 'c3' 열을 기준으로 판별하면 0행과 1행, 3행과 4행의 데이터가 각각 중복된다.  \n",
    "0행과 3행은 처음 나타난 데이터라서 제외하고, 1행과 4행의 데이터만 중복으로 판별하고 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84652e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  c1  c2  c3\n",
      "0  a   1   1\n",
      "2  b   1   2\n",
      "3  a   2   2\n"
     ]
    }
   ],
   "source": [
    "# c2, c3열을 기준으로 중복 행 제거\n",
    "df3 = df.drop_duplicates(subset=['c2', 'c3'])\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38406add",
   "metadata": {},
   "source": [
    "# 데이터 표준화\n",
    "\n",
    "## 단위 환산\n",
    "같은 데이터셋 안에서 서로 다른 측정 단위를 사용한다면, 전체 데이터의 일관성 측면에서 문제가 발생한다.  \n",
    "따라서 측정 단위를 동일하게 맞출 필요가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac593188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name  \n",
      "0       1  chevrolet chevelle malibu  \n",
      "1       1          buick skylark 320  \n",
      "2       1         plymouth satellite  \n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name       kpl  \n",
      "0       1  chevrolet chevelle malibu  7.652571  \n",
      "1       1          buick skylark 320  6.377143  \n",
      "2       1         plymouth satellite  7.652571  \n",
      "\n",
      "\n",
      "    mpg  cylinders  displacement horsepower  weight  acceleration  model year  \\\n",
      "0  18.0          8         307.0      130.0  3504.0          12.0          70   \n",
      "1  15.0          8         350.0      165.0  3693.0          11.5          70   \n",
      "2  18.0          8         318.0      150.0  3436.0          11.0          70   \n",
      "\n",
      "   origin                       name   kpl  \n",
      "0       1  chevrolet chevelle malibu  7.65  \n",
      "1       1          buick skylark 320  6.38  \n",
      "2       1         plymouth satellite  7.65  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# read_csv() 함수로 df 생성\n",
    "df = pd.read_csv('./auto-mpg.csv', header=None)\n",
    "\n",
    "# 열 이름 지정\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "print(df.head(3))\n",
    "\n",
    "# mpg(mile par gallon)를 kpl (kilometer per liter)로 변환(mpg_to_kpl = 0.425)\n",
    "mpg_to_kpl = 1.60934/3.78541\n",
    "\n",
    "# mpg 열에 0.425.를 곱한 결과를 새로운 열(kpl)에 추가\n",
    "df['kpl'] = df['mpg'] * mpg_to_kpl\n",
    "print(df.head(3))\n",
    "print('\\n')\n",
    "\n",
    "# kpl 열을 소수점 아래 둘째자리에서 반올림\n",
    "df['kpl'] = df['kpl'].round(2)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a8e26",
   "metadata": {},
   "source": [
    "## 자료형 변환\n",
    "숫자가 문자열(object)로 저장된 경우에 숫자형(int 또는 float)으로 변환해야한다.  \n",
    "먼저 dtypes 속성을 사용하여 데이터프레임을 구성하는 각 열의 자료형을 확인한다.  \n",
    "dtypes 속성 대신 info() 메소드를 사용해도 되고 각 열의 자료형을 확인할 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "580d6a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpg             float64\n",
      "cylinders         int64\n",
      "displacement    float64\n",
      "horsepower       object\n",
      "weight          float64\n",
      "acceleration    float64\n",
      "model year        int64\n",
      "origin            int64\n",
      "name             object\n",
      "dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read_csv() 함수로 df 생성\n",
    "df = pd.read_csv('./auto-mpg.csv', header=None)\n",
    "\n",
    "# 열 이동 지정\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# 각 열의 자료형 확인\n",
    "print(df.dtypes)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e4226",
   "metadata": {},
   "source": [
    "`horsepower`열은 문자열을 뜻하는 object 자료형이다. 그러나 엔진 출력의 크기를 나타내는 데이터인 만큼 숫자형으로 변환해 주는 것이 적절하다. 엔진 출력이 문자열로 저장된 이유는 `horsepower`열의 고유값을 출력해 보면 알 수 있다. 3번째 줄 중간에 문자열 '?'이 보인다.  \n",
    "고유값 중에 문자 '?'가 섞여 있어서 CSV 파일을 데이터프레임으로 변환하는 과정에서 문자열로 인식된 것으로 보인다. 나머지 값은 모두 숫자형으로 변한하는 것이 적절하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d17982e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['130.0' '165.0' '150.0' '140.0' '198.0' '220.0' '215.0' '225.0' '190.0'\n",
      " '170.0' '160.0' '95.00' '97.00' '85.00' '88.00' '46.00' '87.00' '90.00'\n",
      " '113.0' '200.0' '210.0' '193.0' '?' '100.0' '105.0' '175.0' '153.0'\n",
      " '180.0' '110.0' '72.00' '86.00' '70.00' '76.00' '65.00' '69.00' '60.00'\n",
      " '80.00' '54.00' '208.0' '155.0' '112.0' '92.00' '145.0' '137.0' '158.0'\n",
      " '167.0' '94.00' '107.0' '230.0' '49.00' '75.00' '91.00' '122.0' '67.00'\n",
      " '83.00' '78.00' '52.00' '61.00' '93.00' '148.0' '129.0' '96.00' '71.00'\n",
      " '98.00' '115.0' '53.00' '81.00' '79.00' '120.0' '152.0' '102.0' '108.0'\n",
      " '68.00' '58.00' '149.0' '89.00' '63.00' '48.00' '66.00' '139.0' '103.0'\n",
      " '125.0' '133.0' '138.0' '135.0' '142.0' '77.00' '62.00' '132.0' '84.00'\n",
      " '64.00' '74.00' '116.0' '82.00']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# horsepower 열의 고유값 확인\n",
    "print(df['horsepower'].unique())\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d730cec",
   "metadata": {},
   "source": [
    "`horsepower` 열의 문자열 '?'를 NaN값으로 변환한다. dropna(axis=0) 메소드로 NaN 값이 들어 있는 모든 행을 삭제한다.  \n",
    "이제 `horsepower`열에는 숫자형으로 변환 가능한 값들만 남는다. astype('float')명령을 이용하여 문자열을 실수형으로 변환한다.  \n",
    "실수형 대신 정수형으로 변환하려면, 'float' 대신 'int'를 입력한다. dtypes 속성을 사용하여 변환된 결과가 맞는지 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b0d76fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['horsepower'].replace('?', np.nan, inplace=True) # '?'을 np.nan으로 변경\n",
    "df.dropna(subset=['horsepower'], axis = 0, inplace = True) # 누락 데이터 행 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float') # 문자열을 실수형으로 변환\n",
    "\n",
    "# horsepower 열의 자료형 확인\n",
    "print(df['horsepower'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15607c27",
   "metadata": {},
   "source": [
    "`origin`열에는 정수형 데이터 1, 2, 3이 들어 있지만, 실제로는 국가이름인 'USA, EU, JPN'을 뜻한다.  \n",
    "replace() 메소드를 사용하여 각 숫자 데이터를 국가이름으로 바꿔주면 문자열을 나타내는 object 자료형으로 자동 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b19fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 2]\n",
      "['USA' 'JPN' 'EU']\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# origin 열의 고유값 확인\n",
    "print(df['origin'].unique())\n",
    "\n",
    "# 정수형 데이터를 문자열 데이터로 변환\n",
    "df['origin'].replace({1:'USA', 2:'EU', 3:'JPN'}, inplace = True)\n",
    "\n",
    "# origin 열의 고유값과 자료형 확인\n",
    "print(df['origin'].unique())\n",
    "print(df['origin'].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502221fa",
   "metadata": {},
   "source": [
    "앞에서 'origin' 열의 국가이름은 문자열 데이터(object)이다. 값을 확인해 보면 3개의 국가이름이 계속 반복된다.  \n",
    "이처럼 유한 개의 고유값이 반복적으로 나타나는 경우에는 범주형(category)데이터로 표현하는 것이 효율적이다.  \n",
    "astype('category') 메소드를 이용하면 범주형 데이터로 변환한다. 범주형을 다시 문자열로 변환하려면 astype('str') 메소드를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2051fc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# 문자열을 범주형으로 변환\n",
    "df['origin'] = df['origin'].astype('category')\n",
    "print(df['origin'].dtypes)\n",
    "\n",
    "# 범주형을 문자열로 다시 변환\n",
    "df['origin'] = df['origin'].astype('str')\n",
    "print(df['origin'].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31e25908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358    81\n",
      "39     71\n",
      "105    73\n",
      "Name: model year, dtype: int64\n",
      "271    78\n",
      "376    82\n",
      "107    73\n",
      "Name: model year, dtype: category\n",
      "Categories (13, int64): [70, 71, 72, 73, ..., 79, 80, 81, 82]\n"
     ]
    }
   ],
   "source": [
    "# model year 열의 정수형을 범주형으로 변환\n",
    "print(df['model year'].sample(3))\n",
    "df['model year'] = df['model year'].astype('category')\n",
    "print(df['model year'].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d67973",
   "metadata": {},
   "source": [
    "# 범주형(카테고리) 데이터 처리\n",
    "## 구간 분할\n",
    "데이터 분석 알고리즘에 따라서는 연속 데이터를 그대로 사용하기 보다는 일정한 구간(bin)으로 나눠서 분석하는 것이 효율적인 경우가 있다.  \n",
    "가격, 비용, 효율 등 연속적인 값을 일정한 수준이나 정도를 나타내는 이산적인 값으로 나타내어 구간별 차이를 드러내는 것이다.  \n",
    "이처럼 연속 변수를 일정한 구간으로 나누고, 각 구간을 범주형 이산 변수로 변환하는 과정을 구간 분할(binning)이라고 한다.  \n",
    "판다스 cut() 함수를 이용하면 연속 데이터를 여러 구간으로 나누고 범주형 데이터로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d470ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 46.         107.33333333 168.66666667 230.        ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read_csv() 함수로 df 생성\n",
    "df = pd.read_csv('./auto-mpg.csv', header=None)\n",
    "\n",
    "# 열 이름 지정\n",
    "df.columns = ['mpg', 'cylinders','displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# horsepower 열의 누락 데이터('?')를 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace = True) #'?'을 np.nan으로 변경\n",
    "df.dropna(subset=['horsepower'], axis = 0, inplace=True) # 누락 데이터 행 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float') # 문자열을 실수형으로 변환\n",
    "\n",
    "# np.histogram 함수로 3개의 bin으로 구분할 경계값의 리스트 구하기\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins=3)\n",
    "print(bin_dividers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da97bd",
   "metadata": {},
   "source": [
    "판다스 cut() 함수의 옵션을 설정한다. 앞에서 구한 경계값의 리스트(bin_dividers)를 bins 옵션에 할당하고 각 구간의 이름(bin_names_을 labels옵션에 할당한다.  \n",
    "`include_lowest=True` 옵션을 사용하면 각 구간의 낮은 경계값을 포함한다.  `horsepower`열의 숫자 데이터를 3개의 구간에 할당하고, 각 구간의 이름('저출력', '보통출력', '고출력')으로 입력하여 `hp_bin` 열에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b22cf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    horsepower hp_bin\n",
      "0        130.0   보통출력\n",
      "1        165.0   보통출력\n",
      "2        150.0   보통출력\n",
      "3        150.0   보통출력\n",
      "4        140.0   보통출력\n",
      "5        198.0    고출력\n",
      "6        220.0    고출력\n",
      "7        215.0    고출력\n",
      "8        225.0    고출력\n",
      "9        190.0    고출력\n",
      "10       170.0    고출력\n",
      "11       160.0   보통출력\n",
      "12       150.0   보통출력\n",
      "13       225.0    고출력\n",
      "14        95.0    저출력\n"
     ]
    }
   ],
   "source": [
    "# 3개의 bin에 이름 지정\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "# pd.cut 함수로 각 데이터를 3개의 bin에 할당\n",
    "df['hp_bin'] = pd.cut(x=df['horsepower'], # 데이터 배열\n",
    "                     bins = bin_dividers, # 경계값 리스트\n",
    "                     labels= bin_names, # bin 이름\n",
    "                     include_lowest=True) # 첫 경계값 포함\n",
    "# horsepower 열, hp_bin 열의 첫 15행 출력\n",
    "print(df[['horsepower', 'hp_bin']].head(15))\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481421f1",
   "metadata": {},
   "source": [
    "## 더미 변수\n",
    "앞에서 `horsepower`열의 숫자형 연속 데이터를  `hp_bin` 열의 범주형 데이터로 변환하였다. 하지만 이처럼 카테고리를 나타내는 범주형 데이터를 회귀분석 등 머신러닝 알고리즘에 바로 사용할 수 없는 경우가 있는데, 컴퓨터가 인식 가능한 입력값으로 변환해야 한다.  \n",
    "이럴 때 숫자 0 또는 1로 표현되는 더미 변수(dummy variable)를 사용한다. 여기서 0과 1은 수의 크고 작음을 나타내지 않고, 어떤 특성(feature)이 있는지 없는지 여부만을 표시한다.  \n",
    "범주형 데이터를 컴퓨터가 인식할 수 있도록 숫자 -과 1로만 구성되는 원핫벡터(one hot vector)로 변환한다고 해서 원핫인코딩이라 부른다.  \n",
    "판다스 get_dummies() 함수를 사용하면, 범주형 변수의 모든 고유값을 각각 새로운 더미 변수로 변환한다. 예제에서 `hp_bin`열의 고유값 3개가 각각 새로운 더미 변수 열의 이름이 된다.  각 더미 변수가 본래 속해 있던 행에넌 1이 입력되고, 속하지 않았던 다른 행에는 0이 입력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d2a269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    저출력  보통출력  고출력\n",
      "0     0     1    0\n",
      "1     0     1    0\n",
      "2     0     1    0\n",
      "3     0     1    0\n",
      "4     0     1    0\n",
      "5     0     0    1\n",
      "6     0     0    1\n",
      "7     0     0    1\n",
      "8     0     0    1\n",
      "9     0     0    1\n",
      "10    0     0    1\n",
      "11    0     1    0\n",
      "12    0     1    0\n",
      "13    0     0    1\n",
      "14    1     0    0\n"
     ]
    }
   ],
   "source": [
    "# np.histogram 함수로 3개의 bin으로 구분할 경계값의 리스트 구하기\n",
    "count, bin_dividers = np.histogram(df['horsepower'], bins=3)\n",
    "\n",
    "# 3개의 bin에 이름 지정\n",
    "bin_names = ['저출력', '보통출력', '고출력']\n",
    "\n",
    "# pd.cut으로 각 데이터를 3개의 bin에 할당\n",
    "df['hp_bin'] = pd.cut(x=df['horsepower'], # 데이터 배열\n",
    "                     bins=bin_dividers, # 경계값 리스트\n",
    "                     labels=bin_names, # bin 이름\n",
    "                     include_lowest=True) # 첫 경계값 포함\n",
    "# hp_bin 열의 범주형 데이터를 더미 변수로 변환\n",
    "horsepower_dummies = pd.get_dummies(df['hp_bin'])\n",
    "print(horsepower_dummies.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c835c9",
   "metadata": {},
   "source": [
    "sklearn 라이브러리를 이용해서 원핫인코딩을 편하게 처리할 수 있다.  \n",
    "데이터프레임 df의 `hp_bin` 열에 들어 있는 범주형 데이터를 0,1을 원소로 갖는 원핫벡터로 변환한다.  \n",
    "결과는 선형대수학에서 정의하는 희소행렬(sparse matrix)로 정의된다.  \n",
    "예제에서는 1차원 벡터를 2차원 행렬로 변환하고 다시 희소행렬로 변환한다. 희소행렬은 (행, 열) 좌표와 값 형태로 정리된다.  \n",
    "앞에서 (0,1)은 0행의 1열 위치를 말하고, 데이터 값은 숫자 1.0이 입력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7fc79f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 0 0 1 1 0 2]\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "  (0, 1)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 1)\t1.0\n",
      "  (4, 1)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (6, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (10, 0)\t1.0\n",
      "  (11, 1)\t1.0\n",
      "  (12, 1)\t1.0\n",
      "  (13, 0)\t1.0\n",
      "  (14, 2)\t1.0\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# sklearn 라이브러리 불러오기\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# 전처리를 위한 encoder 객체 만들기\n",
    "label_encoder = preprocessing.LabelEncoder() # label encoder 생성\n",
    "onehot_encoder = preprocessing.OneHotEncoder() # one hot encoder 생성\n",
    "\n",
    "# label encoder로 문자열 범주를 숫자형 범주로 변환\n",
    "onehot_labeled = label_encoder.fit_transform(df['hp_bin'].head(15))\n",
    "print(onehot_labeled)\n",
    "print(type(onehot_labeled))\n",
    "\n",
    "# 2차원 행렬로 변경\n",
    "onehot_reshaped = onehot_labeled.reshape(len(onehot_labeled),1)\n",
    "print(type(onehot_reshaped))\n",
    "\n",
    "# 희소행렬로 변환\n",
    "onehot_fitted =onehot_encoder.fit_transform(onehot_reshaped)\n",
    "print(onehot_fitted)\n",
    "print(type(onehot_fitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42817551",
   "metadata": {},
   "source": [
    "# 정규화\n",
    "각 변수(데이터프레임의 열)에 들어 있는 숫자 데이터의 상대적 크기 차이 때문에 머신러닝 분석결과가 달라질 수 있다. 예를 들어, A 변수는 0-1000 범위의 값을 갖고, B 변수는 0-1 범위의 값을 갖는다고 하자. 아 경우 상대적으로 큰 숫자 값을 갖는 A 변수의 영향이 더 커진다.  \n",
    "따라서 숫자 데이터의 상대적인 크기 차이를 제거할 필요가 있다. 각 열(변수)에 속하는 데이터 값을 동일한 크기 기준으로 나눈 비율로 나타내는 것을 정규화(normalization)라고 한다. 정규화 과정을 거친 데이터의 범위는 0~1 또는 -1~1이 된다.\n",
    "각 열(변수)의 데이터를 해당 열의 최댓값(의 절대값)으로 나누는 방법이 있다. 어떤 열의 원소 값을 그 열의 최대값으로 나누면 가장 큰 값은 최댓값 자기자신을 나눈 1이다. 즉, `horsepower`열의 최댓값은 230인데 최댓값을 정규화 하면 1이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d75f495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    392.000000\n",
      "mean     104.469388\n",
      "std       38.491160\n",
      "min       46.000000\n",
      "25%       75.000000\n",
      "50%       93.500000\n",
      "75%      126.000000\n",
      "max      230.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "0    0.565217\n",
      "1    0.717391\n",
      "2    0.652174\n",
      "3    0.652174\n",
      "4    0.608696\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.454215\n",
      "std        0.167353\n",
      "min        0.200000\n",
      "25%        0.326087\n",
      "50%        0.406522\n",
      "75%        0.547826\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# read_csv() 함수로 df 생성\n",
    "df =  pd.read_csv('./auto-mpg.csv', header=None)\n",
    "\n",
    "# 열 이름 지정\n",
    "df.columns = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight',\n",
    "             'acceleration', 'model year', 'origin', 'name']\n",
    "\n",
    "# horsepower 열의 누락 데이터('?')를 삭제하고 실수형으로 변환\n",
    "df['horsepower'].replace('?', np.nan, inplace=True) # '?'을 np.nan으로 변경\n",
    "df.dropna(subset=['horsepower'], axis=0, inplace=True) # 누락 데이터 행 삭제\n",
    "df['horsepower'] = df['horsepower'].astype('float') # 문자열을 실수형으로 변환\n",
    "\n",
    "# horsepower 열의 통계 요약 정보로 최댓값(max) 확인\n",
    "print(df.horsepower.describe())\n",
    "print('\\n')\n",
    "\n",
    "# horsepower 열의 최댓값의 절대값으로 모든 데이터를 나눠서 저장\n",
    "df.horsepower = df.horsepower/abs(df.horsepower.max())\n",
    "\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccad79c3",
   "metadata": {},
   "source": [
    "각 열(변수)의 데이터 중에서 최댓값(max)과 최솟값(min)을 뺀 값으로 나누는 방법이다.  \n",
    "각 열 데이터에서 해당 열의 최솟값을 뺀 값을 분자로 하고, 해당 열의 최댓값과 최솟값의 차를 분모로 하는 수를 계산하면 가장 큰 값은 역시 1이 된다.  \n",
    "예제에서 `horsepower`열의 최댓값은 230이고 최솟값은 46이다. 최댓값과 최솟값의 차인 184를 이용하여 정규화하면, 최소 0부터 최대 1 사이의 범위로 변환된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3babf7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    392.000000\n",
      "mean       0.454215\n",
      "std        0.167353\n",
      "min        0.200000\n",
      "25%        0.326087\n",
      "50%        0.406522\n",
      "75%        0.547826\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "0    0.456522\n",
      "1    0.646739\n",
      "2    0.565217\n",
      "3    0.565217\n",
      "4    0.510870\n",
      "Name: horsepower, dtype: float64\n",
      "\n",
      "\n",
      "count    392.000000\n",
      "mean       0.317768\n",
      "std        0.209191\n",
      "min        0.000000\n",
      "25%        0.157609\n",
      "50%        0.258152\n",
      "75%        0.434783\n",
      "max        1.000000\n",
      "Name: horsepower, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# horsepower 열의 통계 요약 정보로 최댓값(max)과 최솟값(min) 확인\n",
    "print(df.horsepower.describe())\n",
    "print('\\n')\n",
    "\n",
    "# horsepower 열의 최댓값의 절댓값으로 모든 데이터를 나눠서 저장\n",
    "min_x = df.horsepower - df.horsepower.min()\n",
    "min_max = df.horsepower.max() - df.horsepower.min()\n",
    "df.horsepower = min_x/min_max\n",
    "\n",
    "print(df.horsepower.head())\n",
    "print('\\n')\n",
    "print(df.horsepower.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad2628d",
   "metadata": {},
   "source": [
    "# 시계열 데이터\n",
    "주식, 환율 등 금융데이터를 다루기 위해 개발된 판다스는 시계열 데이터를 다루는 여러가지 유용한 기능을 제공한다. 특히 시계열 데이터를 데이터프레임의 행 인덱스로 사용하면, 시간으로 기록된 데이터를 분석하는 것이 매우 편리하다.  \n",
    "판다스의 시간 표시 방식 중에서 시계열 데이터 표현에 자주 이용되는 두 가지 유형을 알아보자. 특정한 시점을 기록하는 Timestamp와 두 시점 사이의 일정한 기간을 나타내는 Period가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48247f0b",
   "metadata": {},
   "source": [
    "## 다른 자료형을 시계열 객체로 변환\n",
    "우리가 접하는 많은 시간 데이터들은 별도의 시간 자료형(파이썬 datetime 라이브러리 등)으로 기록되지 않고, 문자열 또는 숫자로 저장되는 경우가 많다. 판다스는 다른 자료형으로 저장된 시간 데이터를 판다스 시계열 객체인 Timestamp로 변환하는 함수를 제공하고 있다.\n",
    "- 문자열을 Timestamp로 변환\n",
    " 판다스 to_datetime() 함수를 사용하면 문자열 등 다른 자료형을 판다스 Timestamp를 나타내는 datetime64 자료형으로 변환 가능하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72dd45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume\n",
      "0  2018-07-02  10100  10850  10900  10000  137977\n",
      "1  2018-06-29  10700  10550  10900   9990  170253\n",
      "2  2018-06-28  10400  10900  10950  10150  155769\n",
      "3  2018-06-27  10900  10800  11050  10500  133548\n",
      "4  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    20 non-null     object\n",
      " 1   Close   20 non-null     int64 \n",
      " 2   Start   20 non-null     int64 \n",
      " 3   High    20 non-null     int64 \n",
      " 4   Low     20 non-null     int64 \n",
      " 5   Volume  20 non-null     int64 \n",
      "dtypes: int64(5), object(1)\n",
      "memory usage: 1.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "# read_csv() 함수로 CSV 파일을 가져와서 df로 변환\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "# 데이터 내용 및 자료형 확인\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044ce30c",
   "metadata": {},
   "source": [
    "`Date` 열의 날짜 데이터를 판다스 Timestamp 객체로 바꿔본다.  \n",
    "`Date` 열을 to_datetime()함수의 인자로 전달하면 문자열(object)데이터를 datetime64 자료형으로 변환한다. 변환된 데이터를 `new_Date` 열에 담아서 데이트프레임 df에 추가한다.  \n",
    "info() 메소드로 `new_Date` 열에 들어 있는 데이터 값들이 datetime64 자료형임을 확인할 수 있다. 또한 `new_Date` 열의 개별 원소 데이터를 type() 함수로 확인하면 Timestamp 객체라는 것을 알 수 있다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57f46c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype         \n",
      "---  ------    --------------  -----         \n",
      " 0   Date      20 non-null     object        \n",
      " 1   Close     20 non-null     int64         \n",
      " 2   Start     20 non-null     int64         \n",
      " 3   High      20 non-null     int64         \n",
      " 4   Low       20 non-null     int64         \n",
      " 5   Volume    20 non-null     int64         \n",
      " 6   new_Date  20 non-null     datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(5), object(1)\n",
      "memory usage: 1.2+ KB\n",
      "None\n",
      "\n",
      "\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n"
     ]
    }
   ],
   "source": [
    "# 문자열 데이터(시리즈 객체)를 판다스 Timestamp로 변환\n",
    "df['new_Date'] = pd.to_datetime(df['Date']) # df에 새로운 열로 추가\n",
    "\n",
    "# 데이터 내요ㅗㅇ 및 자료형 확인\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())\n",
    "print('\\n')\n",
    "print(type(df['new_Date'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01fff76",
   "metadata": {},
   "source": [
    "`new_Date` 열을 데이터프레임 df의 행 인덱스로 설정하고, `Date` 열을 제거한다.  \n",
    "이렇게 시계열 값을 행 인덱스로 지정하면 판다스는 DateTimeIndex로 저장한다. 이처럼 시계열 인덱스 클래스를 지원하기 때문에 시간 순서에 맞춰 인덱싱 또는 슬라이싱 하기가 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d77e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Close  Start   High    Low  Volume\n",
      "new_Date                                      \n",
      "2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2018-07-02 to 2018-06-01\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   Close   20 non-null     int64\n",
      " 1   Start   20 non-null     int64\n",
      " 2   High    20 non-null     int64\n",
      " 3   Low     20 non-null     int64\n",
      " 4   Volume  20 non-null     int64\n",
      "dtypes: int64(5)\n",
      "memory usage: 960.0 bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 시계열 값으로 변환된 열을 새로운 행 인덱스로 지정. 기존 날짜 열은 삭제\n",
    "df.set_index('new_Date', inplace=True)\n",
    "df.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# 데이터 내용 및 자료형 확인\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5162367",
   "metadata": {},
   "source": [
    "### Timestamp를 Period로 변환\n",
    "판다스 to_period() 함수를 이용하면 일정한 기간을 나타내는 Period 객체로 Timestamp 객체를 변화할 수 있다.\n",
    "`freq` 옵션에 기준이 되는 기간을 설정한다. 다음의 예제에서 3개의 날짜 데이터를 Timestamp로 변환하고, 기간 옵션을 달리하여 Period 객체를 지정해보자.  \n",
    "DateTimeIndex가 PeriodIndex로 변환되고, 자료형은 datetime64에서 period으로 변환된다. freq 옵션을 `D`로 지정할 경우 1일의 기간을 나타내고 `M`은 1개월의 기간을 뜻한다. `A`는 1년의 기간을 나타내는데, 1년이 끝나는 12월을 기준으로 삼는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14431b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='datetime64[ns]', freq=None)\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01', '2020-03-01', '2021-06-01'], dtype='period[D]', freq='D')\n",
      "PeriodIndex(['2019-01', '2020-03', '2021-06'], dtype='period[M]', freq='M')\n",
      "PeriodIndex(['2019', '2020', '2021'], dtype='period[A-DEC]', freq='A-DEC')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 날짜 형식의 문자열로 구성되는 리스트 정의\n",
    "dates = ['2019-01-01', '2020-03-01', '2021-06-01']\n",
    "\n",
    "# 문자열의 배열(시리즈 객체)을 판다스 Timestamp로 변환\n",
    "ts_dates = pd.to_datetime(dates)\n",
    "print(to_dates)\n",
    "print('\\n')\n",
    "\n",
    "# Timestamp를 Period로 변환\n",
    "pr_day = ts_dates.to_period(freq='D')\n",
    "print(pr_day)\n",
    "pr_month = ts_dates.to_period(freq='M')\n",
    "print(pr_month)\n",
    "pr_year = ts_dates.to_period(freq='A')\n",
    "print(pr_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5190b5",
   "metadata": {},
   "source": [
    "### freq 옵션의 종류\n",
    "\n",
    "|옵션|설명|옵션|설명|\n",
    "|----|----|----|----|\n",
    "|D|day(1일)|B|business day(휴일 제외)|\n",
    "|W|week(1주)|H|hour(1시간)|\n",
    "|M|month end(월말)|T|minute(1분)|\n",
    "|MS| month begin(월초)|S|second(1초)|\n",
    "|Q|quarter end(분기말)|L|millisecond(1/1000초)|\n",
    "|QS|quarter begin(분기초)|U|microsecond(1/1,000,000초)|\n",
    "|A|year end(연말)|N|nanosecond(1/1,000,000,000초)|\n",
    "\n",
    "- 추가적인 것은 판다스 사용자 가이드 <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html>를 참고한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9df1a",
   "metadata": {},
   "source": [
    "## 시계열 데이터 만들기\n",
    "\n",
    "### Timestamp 배열\n",
    "\n",
    "판다스 date_range() 함수를 사용하면 여러 개의 날짜(Timestamp)가 들어 있는 배열 형태의 시계열 데이터를 만들 수 있다. 파이썬 range()함수로 숫자 배열을 만드는 것과 비슷하다.  \n",
    "예제에서 날짜 범위의 시작점으로 '2019-01-01'을 설정하고 날짜 범위의 끝을 따로 정하지 않는다.(end=None), 또한 `periods=6`은 Timestamp를 6개 생성한다는 뜻이다. `freq='MS'`에서 'M'은 월을 뜻하고 'S'는 시작일을 나타낸다. 이 경우 '2019-01-01'부터 한 달 간격으로 각 달의 시작 날짜를 6개 생성한다는 뜻이다. `tz='Asia/Seoul'`은 한국 시간대를 설정하는 옵션이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d43ada14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-01 00:00:00+09:00', '2019-02-01 00:00:00+09:00',\n",
      "               '2019-03-01 00:00:00+09:00', '2019-04-01 00:00:00+09:00',\n",
      "               '2019-05-01 00:00:00+09:00', '2019-06-01 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='MS')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Timestamp의 배열 만들기 - 월 간격, 월의 시작일 기준\n",
    "ts_ms = pd.date_range(start='2019-01-01',\n",
    "                     end = None,\n",
    "                     periods=6,\n",
    "                     freq='MS',\n",
    "                     tz='Asia/Seoul')\n",
    "print(ts_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fb775f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-02-28 00:00:00+09:00',\n",
      "               '2019-03-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-05-31 00:00:00+09:00', '2019-06-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='M')\n",
      "\n",
      "\n",
      "DatetimeIndex(['2019-01-31 00:00:00+09:00', '2019-04-30 00:00:00+09:00',\n",
      "               '2019-07-31 00:00:00+09:00', '2019-10-31 00:00:00+09:00',\n",
      "               '2020-01-31 00:00:00+09:00', '2020-04-30 00:00:00+09:00'],\n",
      "              dtype='datetime64[ns, Asia/Seoul]', freq='3M')\n"
     ]
    }
   ],
   "source": [
    "# 월 간격, 월의 마지막 날 기준\n",
    "ts_me = pd.date_range('2019-01-01', periods=6,\n",
    "                     freq='M',\n",
    "                     tz='Asia/Seoul')\n",
    "print(ts_me)\n",
    "print('\\n')\n",
    "\n",
    "# 분기 (3개월) 간격, 월의 마지막 날 기준\n",
    "ts_3m = pd.date_range('2019-01-01', periods=6,\n",
    "                     freq='3M',\n",
    "                     tz='Asia/Seoul')\n",
    "print(ts_3m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3082c57",
   "metadata": {},
   "source": [
    "### Period 배열\n",
    "판다스 period_range() 함수는 여러 개의 기간(Period)이 들어 있는 시계열 데이터를 만든다.  \n",
    "예제에서 날짜 범위의 시작점으로 '2019-01-01'을 설정하고, 날짜 범위의 끝을 따로 정하지 않는다.(end=None). 그리고 periods=3 옵션은 Period 3개를 만든다는 뜻이다. freq='M'에서 'M'은 월을 나타낸다. 이 경우 PeriodIndex의 원소 '2019-01'은 2019년 1월의 전체 기간을 나타낸다.  \n",
    "나머지 두 원소 '2019-02', '2019-03'도 각각 2월과 3월을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f87c06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeriodIndex(['2019-01', '2019-02', '2019-03'], dtype='period[M]', freq='M')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Period 배열 만들기 - 1개월의 길이\n",
    "pr_m = pd.period_range(start='2019-01-01',\n",
    "                      end = None,\n",
    "                      periods=3,\n",
    "                      freq='M')\n",
    "print(pr_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6b27acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 01:00', '2019-01-01 02:00'], dtype='period[H]', freq='H')\n",
      "\n",
      "\n",
      "PeriodIndex(['2019-01-01 00:00', '2019-01-01 02:00', '2019-01-01 04:00'], dtype='period[2H]', freq='2H')\n"
     ]
    }
   ],
   "source": [
    "# Period 배열 만들기 - 1시간 길이\n",
    "pr_h = pd.period_range(start='2019-01-01',\n",
    "                      end=None,\n",
    "                      periods=3,\n",
    "                      freq=\"H\")\n",
    "print(pr_h)\n",
    "print('\\n')\n",
    "\n",
    "pr_2h = pd.period_range(start= '2019-01-01',\n",
    "                       end=None,\n",
    "                       periods=3,\n",
    "                       freq='2H')\n",
    "print(pr_2h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d0abbe",
   "metadata": {},
   "source": [
    "## 시계열 데이터 활용\n",
    "\n",
    "### 날짜 데이터 분리\n",
    "연-월-일 날짜 데이터에서 일부를 분리하여 추출할 수 있다. 먼저 연-월-일 정보를 연,월,일 각각으로 구분하는 방법부터 알아보자.  \n",
    "날짜 정보가 있는 `Date` 열을 to_datetime()함수에 전달하면 Timestamp로 변환된다.  \n",
    "변화된 결과를 `new_Date`열에 담아서 데이터프레임에 추가한다. dt 속성을 이용하여 `new_Date` 열의 연-월-일 정보에서 연,월,일을 개별적으로 추출한다. dt.year로 추출한 연도는 `year`열에, dt.month로 추출한 월 데이터는 `Month`열에, dt.day로 추출한 일 데이터는 `Day`열에 저장하고, 데이터 프레임에 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "916fe6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26\n",
      "\n",
      "\n",
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  Day\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7    2\n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   29\n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   28\n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   27\n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   26\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "# 문자열인 날짜 데이터 판다스 Timestamp로 변환\n",
    "df['new_Date'] = pd.to_datetime(df['Date']) # df에 새로운 열로 추가\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "\n",
    "# dt 속성을 이용하여 new_Date 열의 연-월-일 정보를 년,월,일로 구분\n",
    "df['Year'] = df['new_Date'].dt.year\n",
    "df['Month'] = df['new_Date'].dt.month\n",
    "df['Day'] = df['new_Date'].dt.day\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9de4fa1",
   "metadata": {},
   "source": [
    "Timestamp 객체를 Period 객체로 변환하는 to_period() 메소드를 적용하여, 연-월-일 중에서 연-월 또는 연도를 추출한다.\n",
    "`Date_yr`열에는 연도를 나타내는 값이 저장되고, `Date_m`열에는 연-월을 나타내는 값이 저장된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a26e0955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Close  Start   High    Low  Volume   new_Date  Year  Month  \\\n",
      "0  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018      7   \n",
      "1  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018      6   \n",
      "2  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018      6   \n",
      "3  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018      6   \n",
      "4  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018      6   \n",
      "\n",
      "   Day Date_yr   Date_m  \n",
      "0    2    2018  2018-07  \n",
      "1   29    2018  2018-06  \n",
      "2   28    2018  2018-06  \n",
      "3   27    2018  2018-06  \n",
      "4   26    2018  2018-06  \n"
     ]
    }
   ],
   "source": [
    "# Timestamp를 Period로 변환하여 연-월-일 표기 변경하기\n",
    "df['Date_yr'] = df['new_Date'].dt.to_period(freq='A')\n",
    "df['Date_m'] = df['new_Date'].dt.to_period(freq='M')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b2242b",
   "metadata": {},
   "source": [
    "추출한 날짜 정보를 데이터프레임의 행 인덱스로 지정할 수도 있다. 예제에서는 `Date_m`열을 데이터프레임의 새로운 행 인덱스로 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61a6b8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Date  Close  Start   High    Low  Volume   new_Date  Year  \\\n",
      "Date_m                                                                     \n",
      "2018-07  2018-07-02  10100  10850  10900  10000  137977 2018-07-02  2018   \n",
      "2018-06  2018-06-29  10700  10550  10900   9990  170253 2018-06-29  2018   \n",
      "2018-06  2018-06-28  10400  10900  10950  10150  155769 2018-06-28  2018   \n",
      "2018-06  2018-06-27  10900  10800  11050  10500  133548 2018-06-27  2018   \n",
      "2018-06  2018-06-26  10800  10900  11000  10700   63039 2018-06-26  2018   \n",
      "\n",
      "         Month  Day Date_yr  \n",
      "Date_m                       \n",
      "2018-07      7    2    2018  \n",
      "2018-06      6   29    2018  \n",
      "2018-06      6   28    2018  \n",
      "2018-06      6   27    2018  \n",
      "2018-06      6   26    2018  \n"
     ]
    }
   ],
   "source": [
    "df.set_index('Date_m', inplace=True)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52509fe5",
   "metadata": {},
   "source": [
    "### 날짜 인덱스 활용\n",
    "Timestamp로 구성된 열을 행 인덱스로 지정하면 DatetimeIndex라는 고유 속성으로 변환된다. 마찬가지로 Period로 구성된 열을 행 인덱스로 지정하면 PeriodIndex라는 속성을 갖는다. 이와 같이 날짜 인덱스를 활용하면 시계열 데이터에 대한 인덱싱과 슬라이싱이 편리하다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08e59f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "DatetimeIndex(['2018-07-02', '2018-06-29', '2018-06-28', '2018-06-27',\n",
      "               '2018-06-26', '2018-06-25', '2018-06-22', '2018-06-21',\n",
      "               '2018-06-20', '2018-06-19', '2018-06-18', '2018-06-15',\n",
      "               '2018-06-14', '2018-06-12', '2018-06-11', '2018-06-08',\n",
      "               '2018-06-07', '2018-06-05', '2018-06-04', '2018-06-01'],\n",
      "              dtype='datetime64[ns]', name='new_Date', freq=None)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read_csv() 함수로 파일을 읽어와 df로 변환\n",
    "df = pd.read_csv('stock-data.csv')\n",
    "\n",
    "# 문자열인 날짜 데이터를 판다스 Timestamp로 변환\n",
    "df['new_Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('new_Date', inplace=True)\n",
    "\n",
    "print(df.head())\n",
    "print('\\n')\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d9888",
   "metadata": {},
   "source": [
    "날짜 인덱스를 사용하는 장점은 연-월-일 중에서 내가 필요로 하는 레벨을 선택적으로 인덱싱할 수 있다는 것이다.  \n",
    "연도만을 기준으로 하거나 연-월 또는 연-월-일 기준으로 선택할 수 있다.  \n",
    "그리고 날짜 범위로 슬라이싱 추출도 가능하다. 예를 들어 df['2018']과 같이 추출하려는 날짜를 입력하면 해당 날짜에 해당하는 모든 행을 선택할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cd40340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "2018-06-29  2018-06-29  10700  10550  10900   9990  170253\n",
      "2018-06-28  2018-06-28  10400  10900  10950  10150  155769\n",
      "2018-06-27  2018-06-27  10900  10800  11050  10500  133548\n",
      "2018-06-26  2018-06-26  10800  10900  11000  10700   63039\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "            Start   High\n",
      "new_Date                \n",
      "2018-07-02  10850  10900\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-07-02  2018-07-02  10100  10850  10900  10000  137977\n",
      "\n",
      "\n",
      "                  Date  Close  Start   High    Low  Volume\n",
      "new_Date                                                  \n",
      "2018-06-25  2018-06-25  11150  11400  11450  11000   55519\n",
      "2018-06-22  2018-06-22  11300  11250  11450  10750  134805\n",
      "2018-06-21  2018-06-21  11200  11350  11750  11200  133002\n",
      "2018-06-20  2018-06-20  11550  11200  11600  10900  308596\n"
     ]
    }
   ],
   "source": [
    "df_y = df['2018']\n",
    "print(df_y.head())\n",
    "print('\\n')\n",
    "df_ym = df.loc['2018-07']\n",
    "print(df_ym)\n",
    "df_ym_cols = df.loc['2018-07', 'Start':'High'] # 열 범위 슬라이싱\n",
    "print(df_ym_cols)\n",
    "print('\\n')\n",
    "df_ymd = df['2018-07-02']\n",
    "print(df_ymd)\n",
    "print('\\n')\n",
    "df_ymd_range = df['2018-06-25':'2018-06-20'] # 날짜 범위 지정\n",
    "print(df_ymd_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06174efc",
   "metadata": {},
   "source": [
    "Timestamp 객체로 표시된 두 날짜 사이의 시간 간격을 구할 수 있다. 예제에서는 어떤 날짜로부터 경과한 날짜를 계산하여 행 인덱스로 지정하고, 데이터를 선택하는 과정까지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f7e87460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Date  Close  Start   High    Low  Volume\n",
      "time_delta                                                \n",
      "180 days    2018-06-28  10400  10900  10950  10150  155769\n",
      "181 days    2018-06-27  10900  10800  11050  10500  133548\n",
      "182 days    2018-06-26  10800  10900  11000  10700   63039\n",
      "183 days    2018-06-25  11150  11400  11450  11000   55519\n",
      "186 days    2018-06-22  11300  11250  11450  10750  134805\n",
      "187 days    2018-06-21  11200  11350  11750  11200  133002\n",
      "188 days    2018-06-20  11550  11200  11600  10900  308596\n",
      "189 days    2018-06-19  11300  11850  11950  11300  180656\n"
     ]
    }
   ],
   "source": [
    "# 시간 간격 계산, 최근 180일~189일 사이의 값들만 선택하기\n",
    "today = pd.to_datetime('2018-12-25') #기준일 생성\n",
    "df['time_delta'] = today - df.index\n",
    "df.set_index('time_delta', inplace=True)\n",
    "df_180 = df['180 days':'189days']\n",
    "print(df_180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac75278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
